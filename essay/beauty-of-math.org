* beauty-of-math
#+TITLE: 数学之美

-----
C3: 统计语言模型 and C5: 隐含马尔可夫模型

- N-1阶马尔可夫假设: 假定文本中的每个词w[i]和前面N-1个词有关, 而与更前面的词无关. 对应的语言模型称为N元模型(N-Gram Model).
- 马尔可夫过程: 随机过程是研究随机变量的时间序列s1,s2...s[t]. 符合马尔可夫假设的随机过程成为马尔可夫过程, 也成为马尔可夫链.
- 隐含马尔可夫模型: 任一时刻t的状态s[t]是不可见的, 所以观察者没有办法通过观察到一个状态序列s1,s2...s[t]来推测转移概率等参数, 但是隐含的状态s1,s2...s[t]是一个马尔可夫链. 同时每个时刻会输出一个符合o[t], 而且o[t]跟s[t]相关且仅跟s[t]相关. o[t]被称为独立假设输出. P(s1,s2...o1,o2...) = \PROD (P(s[t] | s[t-1]) * P(o[t] | s[t])). 针对不同应用,P(s1,s2..o1,o2...)的名称也各不相同: 在语音识别中它被成为"声学模型"(Acoustic Model), 在机器翻译中是"翻译模型"(Translation Model), 而在拼写校正中是"纠错模型"(Correction Model).

-----
C6: 信息的度量和作用

信息熵是对不确定性的衡量, 因此可以想象信息熵能直接用于衡量统计语言模型的好坏. 当然, 因为有了上下文的条件, 所以对高阶的语言模型, 应该用条件熵. 如果再考虑到从训练语料和真实应用的文本中得到的概率函数有偏差, 就需要再引入相对熵的概念.
- 信息熵(Entropy): H(x) = \SUM (-P(x) * log(P(x)))
- 条件熵(Conditional Entropy):
  - P(x,y) # 联合概率分布(Joint Probability)
  - P(x|y) # 条件概率分布(Conditional Probability)
  - H(X|Y) = \SUM (-P(x,y) * log(P(x|y))) # 一个条件
  - H(X|Y,Z) = \SUM (-P(x,y,z) * log(P(x|y,z)))  # 两个条件
  - H(X|Y,Z) <= H(X|Y) <= H(X) 说明X的不确定性在下降
- 互信息: 两个随机时间"相关性"的量化度量
  - I(X;Y) = \SUM (P(x,y) * log(P(x,y) / (P(x) * P(y)))) = H(X) - H(X|Y)
  - 所以如果事件X,Y是相互独立的话, 那么I(X;Y) = 0
- 相对熵(Relative Entropy): 也是用来衡量相关性, 但和变量的互信息不同, 它用来衡量两个取值为正数的函数的相似性
  - KL(f(x) || g(x)) = \SUM (f(x) * log(f(x) / g(x)))
  - Kullback Leibler Divergence.
  - 两个完全相同函数, 相对熵 = 0
  - 相对熵越大, 两个函数差异越大. 反之, 相对熵越小, 两个函数差异越小.
  - 对于概率分布或者概率密度函数, 如果取值均大于0, 相对熵可以度量两个随机分布的差异性.
  - 因为KL(f(x) || g(x)) != KL(g(x) || f(x)), 为了两边计算JS(f(x) || g(x)) = 1/2 * KL(f * g) + 1/2 * KL(g * f)

-----
C7: 贾里尼克和现代语言处理

每当弗莱德和我谈起各自少年时的教育, 我们都同意这样几个观点. 首先, 小学生和中学生其实没有必要花那么多时间读书, 而他们的社会经验, 生活能力, 以及在那时树立起的志向将帮助他们的一生. 第二, 中学阶段花很多时间比同伴多读的课程, 上大学以后用很短时间就能读完, 因为大学阶段, 人的理解能力要强得多. 举个例子, 在中学需要花500小时才能学会的内容, 在大学可能花100小时就够了. 因此, 一个学生在中小学阶段建立的那一点点优势在大学很快就会丧失殆尽. 第三, 学习(和教育)是持续一辈子的过程, 很多中学成绩优异的亚裔学生进入名校后表现明显不如那些出于兴趣而读书的美国同伴, 因为前者持续学习的动力不足. 第四, 书本的内容可以早学, 也可以晚学, 但是错过了成长阶段却是无法补回来的. (因此, 少年班的做法不足取) 现在中国的好学校, 恐怕百分之九十九的孩子在读书上花的时间比我当时要多, 比贾里尼克要多得多, 但是这些孩子今天可能有百分之九十九在学术上的建树不如我, 更不如贾里尼克. 这实在是教育的误区.

贾里尼克从头开始, 在短短两三年内就将CLSP(Center for Language and Speech Processing)变成世界一流的研究中心. 他主要做了两件大事, 两件小事. 两件大事是, 首先, 从美国政府主管研究的部门那里申请到了许多研究经费, 然后, 每年夏天, 他用一部分经费, 邀请世界上20-30名顶级的科学家和兄生到CLSP一起工作, 使得CLSP变成世界上语音和语言处理的中心之一. 两件小事是, 首先他招募了一批当时很有潜力的年轻学者, 第二他利用自己的影响力, 在暑期把他的学生拍到世界上最好的公司去实习, 通过这些学生的优异表现, 树立起CLSP在培养人才方面的声誉.

-----
C8 简单之美-布尔代数和搜索引擎

但是真正做好一件事情是没有捷径, 离不开一万小时的专业训练和努力. 最好搜索, 最基本的要求是每天分析10-20个不好的搜索结果, 累积一段时间才会有感觉. 我在Google改进搜索质量的时候每天分析的搜素数量远不止这个, Google的搜索质量第一技术负责人Amit Singhal至今依然经常分析那些不好的搜索结果. 但是, 很多搜索的工程师(美国的, 中国的都有)都做不到这一点, 他们总是期望靠一个算法, 一个模型就能毕业其功于一役, 而这是不现实的.

-----
C11 如何确定网页和查询的相关性
