* machine-learnin-tom-mitchell
#+TITLE: Machine Learning(机器学习)

-----
引言

一些学科和它们对机器学习的影响
- 人工智能：学习概念的符号表示；作为搜索问题的机器学习；作为提高问题求解能力的学习；利用先验的知识和训练数据一起引导学习。
- 贝叶斯方法：作为计算假设概率基础的贝叶斯法则；朴素贝叶斯；估计未观测到变量值的算法。
- 计算复杂性理论：不同学习任务中固有的复杂性的理论边界，以计算量，训练样例数量，出错数量衡量。
- 控制论：为了优化预定目标，学习对各种处理过程进行控制，学习预测被控制的过程的下一个状态。
- 信息论：熵和信息内容的度量；学习最小描述长度方法；编码假设时，对最佳训练序列的最佳编码及其关系。
- 哲学：奥卡姆的剃刀：最简单的假设是最好的；从观察到的数据泛化的理由分析。
- 心理学和神经生物学：实践的幂定律(power law of practice), 该定律指出对于很大范围内的学习问题，人们的反应速度随着时间次数的幂级提高；激发人工神经网络学习模式的神经生物学研究。
- 统计学：根据有限数据样本，对估计假设精度时出现的误差（例如偏差和方差）的刻画；置信区间，统计检验。

学习任务被简化为发现一个理想目标函数V的可操作描述，通常要完美学习这样一个V的可操作的形式是非常困难的。事实上，我们通常仅希望学习算法得到近似的目标函数，由于这个原因学习目标函数的过程常被称为函数逼近(function approximation).

这书的很多章节给出了一些基本表示（比如线性函数，逻辑描述，决策树，人工神经元网络）定义的假设空间的搜索算法。这些不同的假设表示法适合于学习不同的目标函数。对于其中的每一种假设表示法，对应的学习算法发挥不同内在结构的优势来组织对假设空间的搜索。自始至终，本书够贯穿着这种把学习问题视为搜索问题的看法，从而通过搜索策略和学习器探索的搜索空间的内在结构来刻画学习方法。

-----
概念学习和一般到特殊序

概念学习(concept learning)是指从有关某个布尔函数的输入输出训练样例中推断该布尔函数。概念学习可以看作是搜索预定义潜在假设空间的过程。

实例(instance), 训练样例(training examples), 正例(positive example), 反例(negative example), 所有可能假设(all possible hypotheses)

归纳学习假设：任一假设如果在足够大的训练样例集中很好地逼近目标函数，它也能在未见实例中很好地逼近目标函数。

由于归纳学习需要某种形式的预先假设，或称为归纳偏置(inductive bias), 我们可以用归纳偏置来描述不同学习方法的特征。 # 可以认为归纳偏置描述了这个算法本身在某方面的倾向

-----
决策树学习

通常决策树代表实例属性值约束的合取(conjunction)的析取式(disjunction). 从树根到树叶的每一条路径对影一族属性测试的合取，树本身对应这些合取的析取。

通常决策树学习最适合具有以下几个特征的问题：
- 实例是由“属性-值”对(pair)表示的。简单版本可以处理离散的值，扩展版本可以处理实数值。
- 目标函数具有离散的输出值。扩展算法允许学习具有实数值输出的函数，尽管决策树在这种情况下的应用不太常见。
- 可能需要析取的描述(disjunctive description).
- 训练数据可以包含错误。决策树学习对错误有很好的健壮性。
- 训练数据可以包含缺少属性值的实例。

基本的ID3算法在搜索中不进行回溯，每当在树的某一层次上选择了一个属性进行测试，它不会再回溯重新考虑这个选择。所以它易受无回溯的爬山搜索中的常见风险影响：收敛到局部最优的答案，而不是全局最优。

奥卡姆剃刀(Occam's Razor): 优先选择拟合数据的最简单的假设。一种解释是简单假设的数量少于复杂假设的数量，所以找到一个简单的但是同时与训练数据拟合的假设的可能性较小。

决策树学习的实际问题包括：避免过度拟合数据，处理连续值的属性，属性选择度量标准，处理属性值不完整的训练数据，处理不同代价的属性，提高计算效率。处理连续值的属性可以通过对连续值进行分断或者是映射成为离散值来处理；属性不完整的训练数据可以为缺少值的属性安排最有可能的值，或者是按照概率来赋值；处理不同代价的属性是因为我们取得某些属性的难易程度不同，比如体温相对于血液化验结果更容易获得，在属性筛选方面需要考虑代价函数。

出现过度拟合(overfitting)一种可能原因是训练样例含有随机错误或噪声。当训练数据没有噪声时，过度拟合也可能发生，特别是当很少的样例被关联到叶子节点时。这种情况下，很可能是出现巧合的规律性，使得某些属性恰巧可以很好地被分割样例，但是却与实际的目标函数并无关系。 # 所以在做剪枝时需要减去一些叶子节点上很少的样例的节点。

有几种途径可以被用来避免决策树学习中的过度拟合，它们可以被分为两类：
- 及早停止树增长，在ID3算法完美分类训练数据之前就停止树增长。 # 一种启发式规则：最小描述长度(minimum description length)来指导是否停止树增长. 或者是利用卡方(chi-square)测试来估计进一步扩展节点能否改善整个实例分布上的性能，还是仅仅改善了当前的训练数据上的性能。
- 后修剪法(post-prune), 即允许树过度拟合，然后对整个树进行后修剪。# 通过判断合并某个节点是否能够改善验证数据来决定修剪, 称为错误率降低修剪(reduced-error pruning).
尽管第一种方法看起来更直接，但是对于过度拟合进行后修剪被证明在实践中更成功，因为第一种方法中精确地估计何时停止树增长很困难。
