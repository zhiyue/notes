* machine-learning
** links
- http://guidetodatamining.com/ 偏向于实践的教程
- Statistical Data Mining Tutorials http://www.autonlab.org/tutorials/index.html 

** 机器学习常见算法分类汇总
http://www.kuqin.com/shuoit/20140925/342326.html

-----

按照学习方式划分
- 监督式学习：在监督式学习下，输入数据被称为“训练数据”，每组训练数据有一个明确的标识或结果。在建立预测模型的时候，监督式学习建立一个学习过程，将预测结果与“训练数据”的实际结果进行比较，不断的调整预测模型，直到模型的预测结果达到一个预期的准确率。监督式学习的常见应用场景如分类问题和回归问题。常见算法有逻辑回归（Logistic Regression）和反向传递神经网络（Back Propagation Neural Network）。
- 非监督式学习：在非监督式学习中，数据并不被特别标识，学习模型是为了推断出数据的一些内在结构。常见的应用场景包括关联规则的学习以及聚类等。常见算法包括Apriori算法以及k-Means算法。
- 半监督式学习：在此学习方式下，输入数据部分被标识，部分没有被标识，这种学习模型可以用来进行预测，但是模型首先需要学习数据的内在结构以便合理的组织数据来进行预测。应用场景包括分类和回归，算法包括一些对常用监督式学习算法的延伸，这些算法首先试图对未标识数据进行建模，在此基础上再对标识的数据进行预测。如图论推理算法（Graph Inference）或者拉普拉斯支持向量机（Laplacian SVM.）等。
- 强化学习：在这种学习模式下，输入数据作为对模型的反馈，不像监督模型那样，输入数据仅仅是作为一个检查模型对错的方式，在强化学习下，输入数据直接反馈到模型，模型必须对此立刻作出调整。常见的应用场景包括动态系统以及机器人控制等。常见算法包括Q-Learning以及时间差学习（Temporal difference learning）。
在企业数据应用的场景下， 人们最常用的可能就是监督式学习和非监督式学习的模型。 在图像识别等领域，由于存在大量的非标识的数据和少量的可标识数据， 目前半监督式学习是一个很热的话题。 而强化学习更多的应用在机器人控制及其他需要进行系统控制的领域。

-----

按照算法类似性划分
- 回归算法：回归算法是试图采用对误差的衡量来探索变量之间的关系的一类算法。回归算法是统计机器学习的利器。在机器学习领域，人们说起回归，有时候是指一类问题，有时候是指一类算法，这一点常常会使初学者有所困惑。常见的回归算法包括：最小二乘法（Ordinary Least Square），逻辑回归（Logistic Regression），逐步式回归（Stepwise Regression），多元自适应回归样条（Multivariate Adaptive Regression Splines）以及本地散点平滑估计（Locally Estimated Scatterplot Smoothing）。
- 基于实例的算法：基于实例的算法常常用来对决策问题建立模型，这样的模型常常先选取一批样本数据，然后根据某些近似性把新数据与样本数据进行比较。通过这种方式来寻找最佳的匹配。因此，基于实例的算法常常也被称为“赢家通吃”学习或者“基于记忆的学习”。常见的算法包括 k-Nearest Neighbor(KNN), 学习矢量量化（Learning Vector Quantization， LVQ），以及自组织映射算法（Self-Organizing Map ， SOM）。
- 正则化方法：正则化方法是其他算法（通常是回归算法）的延伸，根据算法的复杂度对算法进行调整。正则化方法通常对简单模型予以奖励而对复杂算法予以惩罚。常见的算法包括：Ridge Regression， Least Absolute Shrinkage and Selection Operator（LASSO），以及弹性网络（Elastic Net）。
- 决策树学习：决策树算法根据数据的属性采用树状结构建立决策模型， 决策树模型常常用来解决分类和回归问题。常见的算法包括：分类及回归树（Classification And Regression Tree， CART）， ID3(Iterative Dichotomiser 3)， C4.5， Chi-squared Automatic Interaction Detection(CHAID), Decision Stump, 随机森林（Random Forest）， 多元自适应回归样条（MARS）以及梯度推进机（Gradient Boosting Machine， GBM）。
- 贝叶斯方法：贝叶斯方法算法是基于贝叶斯定理的一类算法，主要用来解决分类和回归问题。常见算法包括：朴素贝叶斯算法，平均单依赖估计（Averaged One-Dependence Estimators， AODE），以及Bayesian Belief Network（BBN）。
- 基于核的算法：基于核的算法中最著名的莫过于支持向量机（SVM）了。 基于核的算法把输入数据映射到一个高阶的向量空间， 在这些高阶向量空间里， 有些分类或者回归问题能够更容易的解决。 常见的基于核的算法包括：支持向量机（Support Vector Machine， SVM）， 径向基函数（Radial Basis Function ，RBF)， 以及线性判别分析（Linear Discriminate Analysis ，LDA)等。
- 聚类算法：聚类，就像回归一样，有时候人们描述的是一类问题，有时候描述的是一类算法。聚类算法通常按照中心点或者分层的方式对输入数据进行归并。所以的聚类算法都试图找到数据的内在结构，以便按照最大的共同点将数据进行归类。常见的聚类算法包括 k-Means算法以及期望最大化算法（Expectation Maximization， EM）。
- 关联规则学习：关联规则学习通过寻找最能够解释数据变量之间关系的规则，来找出大量多元数据集中有用的关联规则。常见算法包括 Apriori算法和Eclat算法等。
- 人工神经网络：人工神经网络算法模拟生物神经网络，是一类模式匹配算法。通常用于解决分类和回归问题。人工神经网络是机器学习的一个庞大的分支，有几百种不同的算法。（其中深度学习就是其中的一类算法，我们会单独讨论），重要的人工神经网络算法包括：感知器神经网络（Perceptron Neural Network）, 反向传递（Back Propagation）， Hopfield网络，自组织映射（Self-Organizing Map, SOM），学习矢量量化（Learning Vector Quantization， LVQ）。
- 深度学习：深度学习算法是对人工神经网络的发展。在计算能力变得日益廉价的今天，深度学习试图建立大得多也复杂得多的神经网络。很多深度学习的算法是半监督式学习算法，用来处理存在少量未标识数据的大数据集。常见的深度学习算法包括：受限波尔兹曼机（Restricted Boltzmann Machine， RBN）， Deep Belief Networks（DBN），卷积网络（Convolutional Network）, 堆栈式自动编码器（Stacked Auto-encoders）。
- 降低维度算法：像聚类算法一样，降低维度算法试图分析数据的内在结构，不过降低维度算法是以非监督学习的方式试图利用较少的信息来归纳或者解释数据。这类算法可以用于高维数据的可视化或者用来简化数据以便监督式学习使用。常见的算法包括：主成份分析（Principle Component Analysis， PCA），偏最小二乘回归（Partial Least Square Regression，PLS）， Sammon映射，多维尺度（Multi-Dimensional Scaling, MDS）, 投影追踪（Projection Pursuit）等。
- 集成算法：集成算法用一些相对较弱的学习模型独立地就同样的样本进行训练，然后把结果整合起来进行整体预测。集成算法的主要难点在于究竟集成哪些独立的较弱的学习模型以及如何把学习结果整合起来。这是一类非常强大的算法，同时也非常流行。常见的算法包括：Boosting， Bootstrapped Aggregation（Bagging）， AdaBoost，堆叠泛化（Stacked Generalization Blending, SGB），梯度推进机（Gradient Boosting Machine, GBM），随机森林（Random Forest）。

** 《机器学习系统设计》
Building Machine Learning Systems with Python

-----
然而根据亲身经验，我们知道做这些很“酷”的事--使用和调整机器学习算法比如SVM，NNS，或者同时支持两者--其实只需要耗费一位优秀机器学习专家的一点时间。看看下面这个典型的工作流程，你就会发现绝大部分时间花费在一些相当平凡的任务上：1）读取和清洗数据；2）探索和理解输入数据；3）分析如何最好地讲数据呈现给学习算法；4）选择正确的模型和学习算法；5）正确地评估性能。

你通常不会直接将数据输入机器学习算法，而是在训练前对部分数据进行提炼。很多时候，使用机器学习算法会让你得到性能提升的回报。一个简单算法在提炼后数据上的表现，甚至能够超过一个非常复杂的算法在原始数据上的效果。这部分机器学习流程叫做特征工程(feature engineering)，通常是一个非常令人兴奋的挑战。你有创意和智慧，便会立即看到效果。

好特征的目标是在重要的地方取不同值，而在不重要的地方不变。一个很自然就会想到的问题式，我们能否自动滴把好特征选取出来。这个问题叫做特征选择(feature selection). 人们已经提出了很多方法来解决这个问题，但是在实践中，极简单的想法可能已经可以做得很好。

要提升效果，我们基本上有如下选择：1）增加更多的数据[learning_curve]；2）考虑模型复杂度[cross_validation and validation_curve]；3）修改特征空间；4）改变模型。

-----
逻辑回归中的逻辑函数引入是这样的：
- 线性回归的回归函数式 y = w * x
- 逻辑回归中我们使用 log(p / (1-p) 来代替 y.
- 逻辑函数h(x) = p = 1 / (1 + e^{-w * x})

-----
朴素贝叶斯分类器要求所有特征之间相互独立。虽然在实际应用中很少有这种情况，但是在实践中它仍然能够达到非常好的正确率。
- 我们要求解在已知特征F1,F2情况下样本属于某类别C的概率P(C|F1,F2). # 后验概率
- 根据贝叶斯公式P(C|F1,F2) * P(F1,F2) = P(F1,F2|C) * P(C). # P(C)先验概率(prior) P(F1,F2|C)似然性(likehood)
- 预测时因为P(F1,F2)都一样所以我们有时可以不用计算。
- P(F1,F2|C) = P(F1|C) * P(F2|C) 这是因为F1,F2两个特征相互独立。
- 实际过程中可能P(F1,F2) = 0. 那么可以通过加法平滑或是拉普拉斯平滑(laplacian smoothing), 又或是Lidstone平滑来处理。
- 又因为在实际计算时多个p1 * p2...会出现精度问题，所以可以转为log(p1) + log(p2)...来处理。

-----
回归惩罚函数
- Ordinary Least Squares(OLS) 普通最小二乘法，普通线性回归
- L1惩罚(L1范数, L1 norm)则是在OLS上增加a * |w|. Lasso法
- L2惩罚(L2范数, L2 norm)则是在OLS上则加a * |w|^2. Ridge regressin(岭回归)
- L1 + L2则是在OLS上增加a * |w| + b * |w|^2. Elastic Net(弹性网)

-----
整个购物篮分析领域有时又叫做关联规则挖掘(association rule mining). 这些规则式：如果一个顾客购买了X的话，相对于基线，那么他更有可能购买Y。有一个指标来衡量每个规则的价值，称为提升度。提升度就是规则和基线所得到的概率之间的比值：life(X->Y) = P(Y|X) / P(Y). 其中P(Y|X)就是规则对应的概率，而P(Y)则是基线。Apriori是这方面问题的经典算法。

-----
下面这些理由会告诉你为什么在实践中应该尽可能消减维度：
- 多余的特诊会影响或误导学习器。并不是所有机器学习方法都会有这种情况（SVM), 但是大多数模型在维度较小的情况下会比较安全。
- 另一个反对高维特征空间的理由是，更多特征意味着更多参数需要调整，过你喝的风险也越大。
- 我们用来解决问题的数据的维度可能只是虚高，真实维度可能比较小。
- 维度越少意味着训练越快，更多东西可以尝试，能够得到更好的结果。
- 如果我们想要可视化数据，就必须限制在两个或者三个维度上。
